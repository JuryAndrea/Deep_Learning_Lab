# -*- coding: utf-8 -*-
"""Assignment4_helper_code_2022_Jury_Andrea_Donofrio_Consegna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yeuxil8IzfrK1MHcttNa4ljbdgIbACAl
"""

import os
import math
import time
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader


import numpy as np
import matplotlib.pyplot as plt

import pandas as pd

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(DEVICE)

class Vocabulary:

    def __init__(self, pad_token="<pad>", unk_token='<unk>', eos_token='<eos>', sos_token='<sos>'):
        self.id_to_string = {}
        self.string_to_id = {}
        
        # add the default pad token
        self.id_to_string[0] = pad_token
        self.string_to_id[pad_token] = 0
        
        # add the default unknown token
        self.id_to_string[1] = unk_token
        self.string_to_id[unk_token] = 1
        
        # add the default unknown token
        self.id_to_string[2] = eos_token
        self.string_to_id[eos_token] = 2   

        # add the default unknown token
        self.id_to_string[3] = sos_token
        self.string_to_id[sos_token] = 3

        # shortcut access
        self.pad_id = 0
        self.unk_id = 1
        self.eos_id = 2
        self.sos_id = 3

    def __len__(self):
        return len(self.id_to_string)

    def add_new_word(self, string):
        self.string_to_id[string] = len(self.string_to_id)
        self.id_to_string[len(self.id_to_string)] = string

    # Given a string, return ID
    # if extend_vocab is True, add the new word
    def get_idx(self, string, extend_vocab=False):
        if string in self.string_to_id:
            return self.string_to_id[string]
        elif extend_vocab:  # add the new word
            self.add_new_word(string)
            return self.string_to_id[string]
        else:
            return self.unk_id


# Read the raw txt file and generate a 1D pytorch tensor
# containing the whole text mapped to sequence of token ID,
# and a vocab file
class ParallelTextDataset(Dataset):

    def __init__(self, src_file_path, trg_file_path, src_vocab=None,
                 trg_vocab=None, extend_vocab=False, device='cuda'):
        (self.data, self.src_vocab, self.trg_vocab,
         self.src_max_seq_length, self.tgt_max_seq_length) = self.parallel_text_to_data(
            src_file_path, trg_file_path, src_vocab, trg_vocab, extend_vocab, device)

    def __getitem__(self, idx):
        return self.data[idx]

    def __len__(self):
        return len(self.data)

    def parallel_text_to_data(self, src_file, tgt_file, src_vocab=None, tgt_vocab=None,
                          extend_vocab=False, device='cuda'):
        # Convert paired src/tgt texts into torch.tensor data.
        # All sequences are padded to the length of the longest sequence
        # of the respective file.

        assert os.path.exists(src_file)
        assert os.path.exists(tgt_file)

        if src_vocab is None:
            src_vocab = Vocabulary()

        if tgt_vocab is None:
            tgt_vocab = Vocabulary()
        
        data_list = []
        # Check the max length, if needed construct vocab file.
        src_max = 0
        with open(src_file, 'r') as text:
            for line in text:
                tokens = list(line)
                length = len(tokens)
                if src_max < length:
                    src_max = length

        tgt_max = 0
        with open(tgt_file, 'r') as text:
            for line in text:
                tokens = list(line)
                length = len(tokens)
                if tgt_max < length:
                    tgt_max = length
        tgt_max += 2  # add for begin/end tokens
                    
        src_pad_idx = src_vocab.pad_id
        tgt_pad_idx = tgt_vocab.pad_id

        tgt_eos_idx = tgt_vocab.eos_id
        tgt_sos_idx = tgt_vocab.sos_id

        # Construct data
        src_list = []
        print(f"Loading source file from: {src_file}")
        with open(src_file, 'r') as text:
            for line in tqdm(text):
                seq = []
                tokens = list(line)
                for token in tokens:
                    seq.append(src_vocab.get_idx(token, extend_vocab=extend_vocab))
                var_len = len(seq)
                var_seq = torch.tensor(seq, device=device, dtype=torch.int64)
                # padding
                new_seq = var_seq.data.new(src_max).fill_(src_pad_idx)
                new_seq[:var_len] = var_seq
                src_list.append(new_seq)

        tgt_list = []
        print(f"Loading target file from: {tgt_file}")
        with open(tgt_file, 'r') as text:
            for line in tqdm(text):
                seq = []
                tokens = list(line)
                # append a start token
                seq.append(tgt_sos_idx)
                for token in tokens:
                    seq.append(tgt_vocab.get_idx(token, extend_vocab=extend_vocab))
                # append an end token
                seq.append(tgt_eos_idx)

                var_len = len(seq)
                var_seq = torch.tensor(seq, device=device, dtype=torch.int64)

                # padding
                new_seq = var_seq.data.new(tgt_max).fill_(tgt_pad_idx)
                new_seq[:var_len] = var_seq
                tgt_list.append(new_seq)

        # src_file and tgt_file are assumed to be aligned.
        assert len(src_list) == len(tgt_list)
        for i in range(len(src_list)):
            data_list.append((src_list[i], tgt_list[i]))

        print("Done.")
            
        return data_list, src_vocab, tgt_vocab, src_max, tgt_max

# `DATASET_DIR` should be modified to the directory where you downloaded the dataset.
DATASET_DIR = "/content"

TRAIN_FILE_NAME = "train"
VALID_FILE_NAME = "interpolate"

INPUTS_FILE_ENDING = ".x"
TARGETS_FILE_ENDING = ".y"

TASK = "numbers__place_value"
# TASK = "comparison__sort"
# TASK = "algebra__linear_1d"

# Adapt the paths!

src_file_path = f"{DATASET_DIR}/{TASK}/{TRAIN_FILE_NAME}{INPUTS_FILE_ENDING}"
trg_file_path = f"{DATASET_DIR}/{TASK}/{TRAIN_FILE_NAME}{TARGETS_FILE_ENDING}"

train_set = ParallelTextDataset(src_file_path, trg_file_path, extend_vocab=True)

# get the vocab
src_vocab = train_set.src_vocab
trg_vocab = train_set.trg_vocab

print(len(src_vocab))
print(len(trg_vocab))

src_file_path = f"{DATASET_DIR}/{TASK}/{VALID_FILE_NAME}{INPUTS_FILE_ENDING}"
trg_file_path = f"{DATASET_DIR}/{TASK}/{VALID_FILE_NAME}{TARGETS_FILE_ENDING}"

valid_set = ParallelTextDataset(
    src_file_path, trg_file_path, src_vocab=src_vocab, trg_vocab=trg_vocab,
    extend_vocab=False)

batch_size = 64
# batch_size = 128

train_data_loader = DataLoader(
    dataset=train_set, batch_size=batch_size, shuffle=True)

valid_data_loader = DataLoader(
    dataset=valid_set, batch_size=batch_size, shuffle=False)

print(len(train_data_loader.dataset))
print(len(valid_data_loader.dataset))

src_vocab.id_to_string

trg_vocab.id_to_string

########
# Taken from:
# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
# or also here:
# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.0, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.max_len = max_len

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float()
                             * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
        self.register_buffer('pe', pe)  # Will not be trained.

    def forward(self, x):
        """Inputs of forward function
        Args:
            x: the sequence fed to the positional encoder model (required).
        Shape:
            x: [sequence length, batch size, embed dim]
            output: [sequence length, batch size, embed dim]
        """
        assert x.size(0) < self.max_len, (
            f"Too long sequence length: increase `max_len` of pos encoding")
        # shape of x (len, B, dim)
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerModel(nn.Module):
    def __init__(self, source_vocabulary_size, target_vocabulary_size,
                 d_model=256, pad_id=0, encoder_layers=3, decoder_layers=2,
                 dim_feedforward=1024, num_heads=8):
        # all arguments are (int)
        super().__init__()
        self.pad_id = pad_id

        # self.d_model = d_model


        self.embedding_src = nn.Embedding(
            source_vocabulary_size, d_model, padding_idx=pad_id)
        self.embedding_tgt = nn.Embedding(
            target_vocabulary_size, d_model, padding_idx=pad_id)

        self.pos_encoder = PositionalEncoding(d_model)
       
        self.transformer = nn.Transformer(
            d_model, num_heads, encoder_layers, decoder_layers, dim_feedforward)
        
        # ---------------------------------
        self.encoder = self.transformer.encoder
        self.decoder = self.transformer.decoder
        # ---------------------------------
        
        self.linear = nn.Linear(d_model, target_vocabulary_size)
        

    def create_src_padding_mask(self, src):
        # input src of shape ()
        src_padding_mask = src.transpose(0, 1) == 0
        return src_padding_mask

    def create_tgt_padding_mask(self, tgt):
        # input tgt of shape ()
        tgt_padding_mask = tgt.transpose(0, 1) == 0
        return tgt_padding_mask

    
    
    # Implement me!
    def greedy_search(self, src, tgt):
      with torch.no_grad():
        batch_size=src.size(0)
        src_key_padding_mask = self.create_src_padding_mask(src.transpose(0, 1)).to(DEVICE)
        print("src_key_padding_mask", src_key_padding_mask.shape)
        src = self.embedding_src(src.transpose(0, 1))
        src = self.pos_encoder(src)

        out = self.encoder(src = src, src_key_padding_mask = src_key_padding_mask)

        decoded_batch = torch.zeros(tgt.size(0)).to(DEVICE)
        decoded_batch = decoded_batch.reshape(1, tgt.size(0)).to(DEVICE)

        for i in range(tgt.size(1)):
          tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.shape[0]).to(DEVICE)
          tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).to(DEVICE)
          memory_padding_mask = src_key_padding_mask 
          emb_tgt = self.embedding_tgt(tgt)
          enc_tgt = self.pos_encoder(emb_tgt)
          
          # print("tgt", enc_tgt.shape)
          # print("out", out.shape)
          # print("tgt_mask", tgt_mask.shape)
          # print("tgt_key_padding_mask", tgt_key_padding_mask.shape)
          # print("memory_padding_mask", memory_padding_mask.shape)

          output = self.decoder(tgt = enc_tgt, 
                                memory = out, 
                                tgt_mask = tgt_mask,
                                tgt_key_padding_mask = tgt_key_padding_mask,
                                memory_key_padding_mask = memory_padding_mask)   
          
          linear = self.linear(output)
          prediction = linear.argmax(dim=2)

          # stopping criteria
          if prediction.tranpose(0, 1) == trg_vocab.string_to_id['<eos>']:
            break
          if (prediction.T==tgt).all():
            break

          decoded_batch[i, :] = prediction
          decoded_batch = torch.cat([decoded_batch, prediction.T],dim=1)
        
        return decoded_batch

    # Implement me!
    def forward_separate(self, src, tgt):
        """Forward function.

          Parameters:
            src: tensor of shape (sequence_length, batch, data dim)
            tgt: tensor of shape (sequence_length, batch, data dim)
          Returns:
            tensor of shape (sequence_length, batch, data dim)
          """
        src_key_padding_mask = self.create_src_padding_mask(src).to(DEVICE)
        tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).to(DEVICE)
        memory_key_padding_mask = src_key_padding_mask
        tgt_mask = self.Transformer.generate_square_subsequent_mask(tgt.shape[0]).to(DEVICE)

        tgt = self.embedding_tgt(tgt)
        tgt = self.pos_encoder(tgt)
        out = self.embedding_src(src)
        out = self.pos_encoder(out)
        # ---------------------------------
        out = self.encoder(out, src_key_padding_mask = src_key_padding_mask)
        tgt = self.decoder(tgt = tgt, 
                           memory = out, 
                           tgt_mask = tgt_mask,
                           tgt_key_padding_mask = tgt_key_padding_mask,
                           memory_key_padding_mask = memory_key_padding_mask)
        # ---------------------------------
        
        out = self.linear(out)
        return out

    def forward(self, src, tgt):
        """Forward function.

        Parameters:
          src: tensor of shape (sequence_length, batch, data dim)
          tgt: tensor of shape (sequence_length, batch, data dim)
        Returns:
          tensor of shape (sequence_length, batch, data dim)
        """

        src_key_padding_mask = self.create_src_padding_mask(src).to(DEVICE)
        tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).to(DEVICE)
        memory_key_padding_mask = src_key_padding_mask
        tgt_mask = self.transformer.generate_square_subsequent_mask(
            tgt.shape[0]).to(DEVICE)

        tgt = self.embedding_tgt(tgt)
        tgt = self.pos_encoder(tgt)
        out = self.embedding_src(src)
        out = self.pos_encoder(out)
        out = self.transformer(
            out, tgt, src_key_padding_mask=src_key_padding_mask,
            tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask)
        
        out = self.linear(out)


        return out

def eval_model(model):
  validation_accuracy_eval = []
  validation_loss_eval = []
  running_loss_val = 0.0
  running_total_val = 0

  model.eval()
  score = 0
  tloader = tqdm(valid_data_loader)
  with torch.no_grad():
    for i, batch in enumerate(tloader):
      input_batch = batch[0]
      target_batch = batch[1]
      input_batch = input_batch.transpose(0, 1)
      target_batch = target_batch.transpose(0, 1)
      input_batch = input_batch.to(DEVICE)
      targets_in = target_batch[:-1].to(DEVICE)

      output = model(input_batch, targets_in)
      output = output.view(-1, output.shape[-1]).to(DEVICE)

      # targets_out = target_batch[1:].reshape(-1).to(DEVICE)
      targets_out = target_batch[1:]
      targets_out_resh = targets_out.reshape(-1).to(DEVICE)

      loss = loss_fn(output, targets_out_resh)
      running_loss_val += loss.item()
      running_total_val += targets_out_resh.size(0)
      
      validation_loss_eval.append(loss.item())

      _, predicted = output.max(1)
      # predicted = model.greedy_search(
      #           src = input_batch, 
      #           tgt = targets_out)

      for y_true, y_pred in zip(targets_out_resh, predicted.T):
          if (y_true==y_pred).all():
            score+=1
      
      val_acc = 100 * score / running_total_val
      validation_accuracy_eval.append(val_acc)

  return validation_accuracy_eval, validation_loss_eval

model = TransformerModel(source_vocabulary_size = len(src_vocab), 
                         target_vocabulary_size = len(trg_vocab),
                         d_model = 256,
                         pad_id = 0,
                         encoder_layers = 3, 
                         decoder_layers = 2, 
                         dim_feedforward = 1024,
                         num_heads = 8)

# model = TransformerModel(source_vocabulary_size = len(src_vocab), 
#                          target_vocabulary_size = len(trg_vocab),
#                          d_model = 256,
#                          pad_id = 0,
#                          encoder_layers = 2, 
#                          decoder_layers = 1, 
#                          dim_feedforward = 1024,
#                          num_heads = 8)


model = model.to(DEVICE)  

n_epochs = 1
learning_rate = 0.0001

loss_fn = nn.CrossEntropyLoss(ignore_index = 0)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

training_loss = []
mean_training_loss = []

validation_loss = []
mean_validation_loss = []

training_accuracy = []
mean_training_accuracy = []

validation_accuracy = []
mean_validation_accuracy = []

score = 0
pos = 0

for epoch in range(n_epochs):
    
    running_loss = 0.0
    running_total = 0

    tloader = tqdm(train_data_loader)
    for i, batch in enumerate(tloader):
        model.train()

        input_batch = batch[0]
        target_batch = batch[1]
        input_batch = input_batch.transpose(0, 1)
        target_batch = target_batch.transpose(0, 1)

        input_batch = input_batch.to(DEVICE)
        targets_in = target_batch[:-1].to(DEVICE)

        output = model(input_batch, targets_in)
        output = output.view(-1, output.shape[-1]).to(DEVICE)
        targets_out = target_batch[1:].reshape(-1).to(DEVICE)

        loss = loss_fn(output, targets_out)
        running_loss += loss.item()
        running_total += targets_out.size(0)
        training_loss.append(loss.item())

        
        if i % 10 == 0:
          optimizer.zero_grad()
          loss.backward()
          torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
          optimizer.step() 

        with torch.no_grad():
            _, predicted = output.max(1)


        for y_true, y_pred in zip(targets_out, predicted.T):
          if (y_true==y_pred).all():
            score+=1

        train_acc = 100 * score / running_total
        training_accuracy.append(train_acc)

        if i % 1000 == 0:
          validation_accuracy , validation_loss = eval_model(model)
          mean_training_accuracy.append(np.mean(training_accuracy))
          mean_training_loss.append(np.mean(training_loss))
          mean_validation_loss.append(np.mean(validation_loss))
          mean_validation_accuracy.append(np.mean(validation_accuracy))

          # print()
          print(f'epoch: {epoch}, steps: {i}, '
                f'mean train_loss: {mean_training_loss[pos]:.3f}, '
                f'mean train_acc: {mean_training_accuracy[pos]:.1f}% '
                f'mean valid_loss: {mean_validation_loss[pos]:.3f}, '
                f'mean valid_acc: {mean_validation_accuracy[pos]:.1f}')
          # print()
          validation_accuracy = []
          validation_loss = []
          training_accuracy = []
          training_loss = []
          running_loss = 0.0
          running_total = 0
          pos += 1
          score = 0

# plot
fig, ax = plt.subplots()
ax.plot(mean_training_loss, linewidth=2, label='Training loss')
ax.plot(mean_validation_loss, linewidth=2, label='Validation loss')
ax.legend()
ax.set_xlabel("Epochs")
ax.set_ylabel("mean epoch's loss")
plt.title("Training and validation losses over epochs")
plt.show()

fig, ax = plt.subplots()
ax.plot(mean_training_accuracy, linewidth=2, label='Training accuracy')
ax.plot(mean_validation_accuracy, linewidth=2, label='Validation accuracy')
ax.legend()
ax.set_xlabel("Epochs")
ax.set_ylabel("mean epoch's accuracy")
plt.title("Training and validation accuracies over epochs")
plt.show()