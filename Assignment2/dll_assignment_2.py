# -*- coding: utf-8 -*-
"""dll_assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NLNS9MSJ5W8ZBvkNKT7bT26Nwojm2N9j
"""

import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim

import torchvision
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.transforms as transforms_2

from torchsummary import summary

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(device)

#1.1.2
transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
    ])

#1.1.1
train_set = torchvision.datasets.CIFAR10(
    root='./data', train=True, 
    transform = transforms, 
    download=True)

test_set = torchvision.datasets.CIFAR10(
    root='./data', train=False, transform=transforms)

print("len train set: ", len(train_set))
print('len test set: ', len(test_set))

for i in range(4):
  img = np.transpose(train_set[i][0].numpy(), (1, 2, 0))
  plt.imshow(img)
  plt.show()

# for training:
batch_size = 32
learning_rate = 0.002
momentum = 0.9

#1.1.4
idx = np.arange(len(train_set))

# Use last 1000 images for validation
val_indices = idx[50000-1000:]
train_indices= idx[:-1000]

print(len(val_indices))
print(len(train_indices))

mean_train_set = torchvision.datasets.CIFAR10(
    root='./data', train=True, 
    transform = transforms_2.ToTensor(), 
    download=True)

#1.1.3
imgs = [item[0] for item in mean_train_set] # item[0] and item[1] are image and its label
imgs = torch.stack(imgs, dim=0).numpy()

# calculate mean over each channel (r,g,b)
mean_r = imgs[:,0,:,:].mean()
mean_g = imgs[:,1,:,:].mean()
mean_b = imgs[:,2,:,:].mean()
print('mean for (r, g, b): ', round(mean_r, 4),round(mean_g, 4),round(mean_b, 4))

# calculate std over each channel (r,g,b)
std_r = imgs[:,0,:,:].std()
std_g = imgs[:,1,:,:].std()
std_b = imgs[:,2,:,:].std()
print('std for (r, g, b): ',round(std_r, 3),round(std_g, 3),round(std_b, 3))

train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)
valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)

train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,
                                          sampler=train_sampler, num_workers=2)

valid_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,
                                          sampler=valid_sampler, num_workers=2)

test_loader = torch.utils.data.DataLoader(
    dataset=test_set, batch_size=batch_size, shuffle=False)

class FFModel(nn.Module):
    def __init__(self):
      super(FFModel, self).__init__()
      self.conv1 = nn.Conv2d(3, 32, 3)
      self.conv2 = nn.Conv2d(32, 32, 3)

      self.pool = nn.MaxPool2d(2)
      
      self.conv3 = nn.Conv2d(32, 64, 3)
      self.conv4 = nn.Conv2d(64, 64, 3)
      
      self.fc1 = nn.Linear(64*5*5, 512)
      
      self.fc2 = nn.Linear(512, 10)
      
      self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
      x = F.relu(self.conv1(x))
      x = F.relu(self.conv2(x))

      x = self.pool(x)
      x = self.dropout(x)

      x = F.relu(self.conv3(x))
      x = F.relu(self.conv4(x))
      
      x = self.pool(x)
      x = self.dropout(x)
      
      x = x.view(-1, 64*5*5)
      x = self.fc1(x)
      
      x = self.dropout(x) # added

      x = F.relu(x)
      
      out = self.fc2(x)
      return out

model = FFModel()

model = model.to(device)  # put all model params on GPU.

# Create loss and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)

print(summary(model, (3, 32, 32)))

training_loss = []
mean_training_loss = []

validation_loss = []
mean_validation_loss = []

training_accuracy = []
mean_training_accuracy = []


validation_accuracy = []
mean_validation_accuracy = []

# Training

best_val_acc = 0
best_epoch = 0
num_epochs = 100

for epoch in range(1, num_epochs):
    running_loss = 0.0
    running_total = 0
    running_correct = 0
    run_step = 0
    for i, (images, labels) in enumerate(train_loader):
        model.train()  # put the model to train mode
        images = images.to(device)
        labels = labels.to(device)  
        outputs = model(images) 
        loss = loss_fn(outputs, labels)
        optimizer.zero_grad()  # reset gradients.
        loss.backward()  # compute gradients.
        optimizer.step()  # update parameters
        running_loss += loss.item()
        running_total += labels.size(0)      
        
        training_loss.append(loss.item())

        with torch.no_grad():
            _, predicted = outputs.max(1)
        running_correct += (predicted == labels).sum().item()
        run_step += 1
        train_acc = 100 * running_correct / running_total
        training_accuracy.append(train_acc)
        if i % 200 == 0:
            # check accuracy.
            print(f'epoch: {epoch}, steps: {i}, '
                  f'train_loss: {running_loss / run_step :.3f}, '
                  f'running_acc: {100 * running_correct / running_total:.1f} %')
            running_loss = 0.0
            running_total = 0
            running_correct = 0
            run_step = 0

    
    mean_training_loss.append(np.mean(training_loss))
    mean_training_accuracy.append(np.mean(training_accuracy))
    

    # validation
    correct = 0
    total = 0
    val_acc = 0
    model.eval()
    with torch.no_grad():
      for data in valid_loader:
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)

        loss = loss_fn(outputs, labels)
        validation_loss.append(loss.item())

        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        val_acc = 100 * correct / total
        validation_accuracy.append(val_acc)

    print(f'Validation accuracy: {100 * correct / total: .1f} %')
    
    mean_validation_loss.append(np.mean(validation_loss))
    mean_validation_accuracy.append(np.mean(validation_accuracy))

    if val_acc > best_val_acc:
      best_val_acc = val_acc
      best_epoch = epoch

print('The best validation accurracy is: ', best_val_acc, ' and it epoch is: ', best_epoch)
print('Finished Training')

print(len(mean_training_loss))
print(len(mean_validation_loss))

fig, ax = plt.subplots()
ax.plot(mean_training_loss, linewidth=2, label='Training loss')
ax.plot(mean_validation_loss, linewidth=2, label='Validation loss')
ax.legend()
ax.set_xlabel("Epochs")
ax.set_ylabel("mean epoch's loss")
plt.title("Training and validation losses over epochs")
plt.show()

print(len(mean_training_accuracy))
print(len(mean_validation_accuracy))

fig, ax = plt.subplots()
ax.plot(mean_training_accuracy, linewidth=2, label='Training accuracy')
ax.plot(mean_validation_accuracy, linewidth=2, label='Validation accuracy')
ax.legend()
ax.set_xlabel("Epochs")
ax.set_ylabel("mean epoch's accuracy")
plt.title("Training and validation accuracies over epochs")
plt.show()

# Evaluation
# Dataloaders
test_loader = torch.utils.data.DataLoader(
    dataset=test_set, batch_size=batch_size, shuffle=False)

with torch.no_grad():
    correct = 0
    total = 0
    model.eval() # Set model in eval mode.
    for data in test_loader:
        images, labels = data
        
        images = images.to(device)
        labels = labels.to(device) 
        outputs = model(images) 
        # ’outputs’ are logits (unnormalized log probability).
        # Model prediction is the class which has the highest 
        # probability according to the model ,
        # i.e. the class which has the highest logit value:
        _, predicted = outputs.max(dim=1)
        
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    test_acc = 100 * correct / total
    print(f'Test accuracy: {test_acc} %')
    print(f'Test error rate: {100 - 100 * correct / total: .2f} %')

from torch.nn.functional import softmax
def num_to_name(index):
  switcher = {
        0: "plane",
        1: "car",
        2: "bird",
        3: "cat",
        4: "deer",
        5: "dog",
        6: "frog",
        7: "horse",
        8: "ship",
        9: "truck"
  }
  return switcher.get(index, "nothing")
  
for i in range(7):
  data, labels = next(iter(valid_loader))
  img = data[0]
  label = labels[0]

  np_img = np.transpose(img.numpy(), (1, 2, 0))
  #to improve the visualization of img 
  #I move the average and compress the img to reduce clipping
  plt.imshow((np_img+0.46)*0.6)
  plt.show()
  with torch.no_grad():
    output = model(data.to(device))
    output = output.cpu()[0]
    probs = softmax(output, dim=0)

  print(f'output probability distribution: \n{probs}')
  print(f'max probability distribution is in position: {np.argmax(probs).item()}, value = {max(probs).item():.3f}')
  print(f'prediction = {num_to_name(np.argmax(probs).item())}, label = {num_to_name(label.item())}\n')